# -*- coding: utf-8 -*-
"""CNF.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ESap9WTjOovYn-N0GsbwcsljOwA1z5Fn
"""

!pip install torchinfo
!pip install nflows

import torch
import torchinfo
import numpy as np
import sklearn
import matplotlib.pyplot as plt
import tqdm
from torch.utils.data import DataLoader
import time

from nflows.flows.base import Flow  # a container for full Flow
from nflows.distributions.normal import StandardNormal  # Gaussian latent space distribution
from nflows.transforms.base import (
    CompositeTransform,
)  # a wrapper to stack simpler transformations to form a more complex one
from nflows.transforms.autoregressive import (
    MaskedAffineAutoregressiveTransform,
)  # the basic transformation, which we will stack several times
from nflows.transforms.autoregressive import (
    MaskedPiecewiseRationalQuadraticAutoregressiveTransform,
)  # the basic transformation, which we will stack several times
from nflows.transforms.permutations import ReversePermutation # a layer that simply reverts the order of outputs

print(torch.cuda.is_available())
print(torch.cuda.device_count())
dnumber = 0
device = torch.device(f"cuda:{dnumber}" if torch.cuda.is_available() else "cpu")
print(device)
device_name = torch.cuda.get_device_name(dnumber)
print(device_name)

def gauss(N,mu,sig,x):
    N_t = N.reshape(-1,1)
    mu_t = mu.reshape(-1,1)
    sig_t = sig.reshape(-1,1)

    term1 = N_t / (sig_t * np.sqrt(2 * np.pi))
    term2 = np.exp(-0.5 * np.square((x - mu_t) / sig_t))
    return np.array(term1 * term2)


def generatePrior(sampleSize):
    N1 = np.random.uniform(10,50,sampleSize)
    N2 = np.random.uniform(10,30,sampleSize)

    mu1 = np.random.uniform(1,3,sampleSize)
    mu2 = np.random.uniform(5,9,sampleSize)

    sig1 = np.random.uniform(1,3,sampleSize)
    sig2 = np.random.uniform(5,9,sampleSize)

    return N1,mu1,sig1,N2,mu2,sig2


def generateTrainingData(sampleNumber):
  N1,mu1,sig1,N2,mu2,sig2 = generatePrior(sampleNumber)

  raw = np.arange(0.5,10,1) #startbinCenter, endBinEdge, StepSize [0.5,1.5...9.5]
  gaussTotal = gauss(N1,mu1,sig1,raw) + gauss(N2,mu2,sig2,raw)

  binNumber = len(gaussTotal)
  dataPoisson = np.random.poisson(lam=gaussTotal,size=None)

  #thetaData = np.column_stack((N1,mu1,sig1,N2,mu2,sig2))
  return dataPoisson

dataPoisson = generateTrainingData(10000000)

"""I am going to normalize the data using min-max I can preserve the relationships while keeping it ideal for sigmoid to take part perhaps. I also found appraches involving log, if this does not work I will look into that."""

max_vals = torch.tensor(np.max(dataPoisson, axis = 0))
min_vals = torch.tensor(np.min(dataPoisson, axis = 0))
input_p = torch.tensor(dataPoisson,dtype=torch.float32)

"""We may have to rethink structure of data storage for CNF later. But for now the autoencoder simply needs to take the 10-D output in and minimize loss."""

Datasets = DataLoader(input_p, batch_size =4096, shuffle=True)

#for data in Datasets:
#  print(data.shape)

class autoEncoder(torch.nn.Module):
  def __init__(self, input_dim, hidden_dim):
    super().__init__()

    #self.max = torch.nn.Parameter(max_vals,requires_grad=False)
    #self.min = torch.nn.Parameter(min_vals,requires_grad=False)

    self.register_buffer("min", min_vals)
    self.register_buffer("max", max_vals)



    self.encoder = torch.nn.Sequential(
        torch.nn.Linear(input_dim,32),
        torch.nn.ReLU(),
        torch.nn.Linear(32,hidden_dim),
    )

    self.decoder = torch.nn.Sequential(
        torch.nn.Linear(hidden_dim,32),
        torch.nn.ReLU(),
        torch.nn.Linear(32,input_dim)
    )

  def forward(self,x):
    x = (x-self.min)/(self.max - self.min + 1e-8)

    encode = self.encoder(x)
    decode = self.decoder(encode)

    y = decode*(self.max - self.min) + self.min
    return y

lr = 1e-4
inputDim = 10
hiddenDim = 6
compress = autoEncoder(inputDim, hiddenDim).to(device)
optim = torch.optim.Adam(compress.parameters(), lr=lr)
loss_fn = torch.nn.MSELoss()
torchinfo.summary(compress)

max_iter = 5
losses = []


for iter in tqdm.tqdm(range(max_iter)):
  iter_losses = []
  for x_batch in Datasets:
      x_batch = x_batch.to(device, non_blocking=True)
      optim.zero_grad()
      y_pred = compress(x_batch)
      loss = loss_fn(y_pred, x_batch)
      iter_losses.append(loss.item())
      loss.backward()
      optim.step()

  losses.append(np.mean(np.array(iter_losses)))

print(len(losses))
plt.plot(losses)

print(losses[-1])

testP = torch.tensor(generateTrainingData(1), dtype = torch.float32)
testP = testP.to(device)
decodePtest = compress(testP)

decodePtest = compress(testP).to("cpu").detach().numpy().flatten()
testP_CPU = testP.to("cpu").detach().numpy().flatten()

print(decodePtest)
print(testP_CPU)

bins = np.array(range(11))
plt.hist(bins[:-1], bins,weights=testP_CPU,color='blue',edgecolor='black',alpha=0.5,label="TrueBin")
plt.hist(bins[:-1], bins,weights=decodePtest,color='red',edgecolor='black',alpha=0.4,label="decodedBin")
plt.legend()

